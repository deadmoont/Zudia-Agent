##ocr_upload.py

import fitz  # pip install pymupdf
# from ..utils.ocr_utils import extract_clauses, extract_metadata
# from ..models.document_models import ClauseContent
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Union
import json
import os
from typing import List, Union, Dict, Any
# import json
from typing import List
import uuid
import faiss
from fastapi import APIRouter, UploadFile, File, HTTPException
from utils.db_client import get_database

# ############REMOVE DOCUMENT_MODELS2############
# backend/models/document_models.py (Simplified Clause Model)
from pydantic import BaseModel, Field
from typing import List, Optional
from pydantic import BaseModel, Field
from datetime import date # Essential for date fields

class ClauseContent(BaseModel):
    """
    Schema for individual extracted clauses, used for the flat list.
    """
    clause_id: str = Field(
        description="The unique identifier for the clause (e.g., '1.1', '8.1.c')."
    )
    # The 'title' field will be generated by the LLM as a short descriptive name
    title: str = Field(
        description="A short, descriptive title for the clause (e.g., 'Confidentiality Obligations')."
    )
    text: str = Field(
        description="The full, verbatim text of the clause. Do not summarize."
    )

class ClausesListContainer(BaseModel):
    """
    A temporary container to hold the list of ClauseContent instances 
    when performing the single LLM extraction call.
    """
    clauses: List[ClauseContent] = Field(
        description="A list containing all extracted ClauseContent objects from the document."
    )   

class DocumentMetadata(BaseModel):
    """Schema for extracting top-level metadata and properties from the PDF text."""
    doc_id: str = Field(description="A system-generated unique ID for this document (e.g., UUID or sequential number).")
    doc_name: str = Field(description="The descriptive common name of the document (e.g., 'Deloitte NDA 2023').")
    doc_ref_no: Optional[str] = Field(description="The external reference number or template ID (e.g., 'DLCS-01-04').")
    doc_type: str = Field(description="The category of the document (e.g., 'NDA', 'Service Agreement').")
    entity_1_name: str = Field(description="Name of the First Party/Principal.")
    entity_2_name: str = Field(description="Name of the Second Party/Service Provider.")
    execution_date: Optional[date] = Field(description="The date the agreement was legally executed (YYYY-MM-DD format).")
    term_length: Optional[str] = Field(description="The initial tenure or duration (e.g., '2 years', 'perpetuity').")
    termination_notice: Optional[str] = Field(description="The required notice period for termination without cause (e.g., '6 months', '30 days').")
    fee_structure: Optional[str] = Field(description="How fees are defined (e.g., 'Fixed Monthly Retainer', 'Per Schedule B').")
    confidentiality_survival: Optional[str] = Field(description="How long confidentiality obligations remain post-termination (e.g., 'perpetuity', '2 years').")    

###########################################################################################################################################
#########################################REMOVE GEMINI CLIENT UTIL#################################################################
import os
import json
from google import genai
from google.genai import types
from pydantic import BaseModel
from typing import Union, Dict

# âœ… Initialize Gemini client once
os.environ["GEMINI_API_KEY"] = os.getenv("GEMINI_API_KEY", "AIzaSyBNHeDNS2hJ5rc68Zs_woomjeCdlBe5wQU")
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])

def generate_from_gemini(prompt: str, model: str = "gemini-2.5-flash") -> str:
    """
    Generate text output using Gemini 2.5 Flash.
    Returns plain text content.
    """
    try:
        response = client.models.generate_content(
            model=model,
            contents=prompt
        )
        return response.text
    except Exception as e:
        print("Gemini API error:", e)
        return f"Error: {e}"
    
def generate_structured_content(prompt: str, schema: BaseModel) -> Union[str, Dict]:
    """
    Sends a prompt to Gemini and forces the response into a structured JSON format 
    defined by the provided Pydantic BaseModel (schema).

    Returns: A JSON string of the structured data, or a dictionary containing an error message.
    """
    
    if client is None:
        return {"error": "API_NOT_INITIALIZED", "message": "Gemini client failed initialization."}
    
    # 1. Define the generation configuration with the response schema
    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=schema,
    )
    
    # 2. Call the API
    try:
        response = client.models.generate_content(
            model='gemini-2.5-pro', # Use Pro for complex, multi-level JSON extraction
            contents=prompt,
            config=config,
        )
        
        # 3. Return the result
        # The response.text field contains the validated JSON string returned by the model.
        return response.text 

    except types.InternalServerError as e:
        # Catch specific API errors (e.g., server overload, temporary issue)
        print(f"Gemini Internal Server Error: {e}")
        return {"error": "API_ERROR", "message": "Gemini API returned an internal error."}
    except Exception as e:
        # Catch general errors (e.g., network timeout, invalid schema usage)
        print(f"Gemini structured extraction failed: {e}")
        return {"error": "EXTRACTION_FAILED", "message": f"General failure: {str(e)}"}
# ###########################################################################################################################


# #######################REMOVE OCR_UTILS##########################################################################
# backend/utils/ocr_utils.py

from typing import Dict, Any
import json
from typing import List
# from ..models.document_models import ClauseContent, ClausesListContainer, DocumentMetadata
# from .gemini_client import generate_structured_content # Assuming gemini_client is in the same utils directory

def extract_clauses(raw_pdf_text: str) -> List[ClauseContent]:
    """
    Calls Gemini to perform content segregation of the raw text and returns 
    a list of ClauseContent instances.
    """
    
    # 1. Construct the detailed prompt
    prompt = f"""
    You are an expert legal document content extractor. Analyze the full agreement text below. Your task is to identify all relevant legal provisions, extract their verbatim content, and assign each a short, descriptive title

    **STRICTLY adhere to the JSON schema provided.**
    
    DOCUMENT TEXT:
    ---
    {raw_pdf_text}
    ---
    """
    
    # 2. Call the structured generation utility
    # This utility returns a JSON string that conforms to the ClausesListContainer schema.
    structured_data_json_string = generate_structured_content(
        prompt=prompt,
        schema=ClausesListContainer 
    )
    
    # 3. Deserialize and validate the result
    try:
        # Check for error message returned from utility if the Gemini call failed
        if "error" in structured_data_json_string:
             print(f"Gemini API Error: {structured_data_json_string.get('message', 'Check client logs.')}")
             return []

        # Parse the JSON string
        data = json.loads(structured_data_json_string) 
        
        # Extract the list from the 'clauses' key
        clauses_data = data.get('clauses', [])

        # Validate and convert the raw dicts into ClauseContent Pydantic instances
        # This provides final validation that the LLM output is correct.
        clause_instances = [ClauseContent(**c) for c in clauses_data]
        
        return clause_instances

    except json.JSONDecodeError as e:
        print(f"JSON Parsing Error after Gemini call (Invalid JSON format): {e}")
        return []
    except Exception as e:
        # This catches errors like Pydantic validation failure if data types were incorrect
        print(f"Error during clause instance creation/validation: {e}")
        return []


def extract_metadata(raw_pdf_text: str, system_doc_id: str) -> Dict[str, Any]:
    """
    Calls Gemini to extract key top-level properties from the raw text.
    
    Args:
        raw_pdf_text: The full text of the document.
        system_doc_id: A UUID or unique ID generated by the system (e.g., "001").
        
    Returns: A dictionary of the extracted metadata fields.
    """
    
    # 1. Construct the detailed prompt
    prompt = f"""
    You are an expert legal metadata extractor. Analyze the full agreement text below.
    Extract the following single-value properties: Document Name, Reference Number, 
    Entity Names, Execution Date (as YYYY-MM-DD), Term Length, Termination Notice, 
    Fee Structure, and Confidentiality Survival.
    
    **STRICTLY adhere to the JSON schema provided.**
    
    DOCUMENT TEXT:
    ---
    {raw_pdf_text}
    ---
    """
    
    # 2. Call the structured generation utility
    structured_data_json_string = generate_structured_content(
        prompt=prompt,
        schema=DocumentMetadata # Use the metadata model
    )
    
    # 3. Deserialize and validate the result
    try:
        # Handle potential API errors before loading JSON
        if isinstance(structured_data_json_string, dict) and "error" in structured_data_json_string:
             return structured_data_json_string
             
        # Load the structured JSON into a dictionary
        metadata_dict = json.loads(structured_data_json_string) 
        
        # Inject the system-generated doc_id (which Gemini does not generate)
        metadata_dict['doc_id'] = system_doc_id

        # Validate the dictionary against the Pydantic model once more (optional but safe)
        DocumentMetadata(**metadata_dict)
        
        return metadata_dict

    except json.JSONDecodeError as e:
        print(f"Metadata JSON Parsing Error: {e}")
        return {"error": "PARSING_FAILED", "message": "Failed to decode Gemini's metadata JSON."}
    except Exception as e:
        print(f"Metadata Validation Error: {e}")
        return {"error": "VALIDATION_FAILED", "message": "Extracted data did not match schema."}
# ############################################################################################################################    

import fitz  # pip install pymupdf
# from ..utils.ocr_utils import extract_clauses, extract_metadata
# from ..models.document_models import ClauseContent
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Union
import json
import os
from typing import List, Union, Dict, Any
# import json
from typing import List
import uuid
import faiss
from fastapi import APIRouter, UploadFile, File, HTTPException



EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
EMBEDDING_MODEL = None
EMBEDDING_DIMENSION = 0
EMBEDDING_MODEL = SentenceTransformer(EMBEDDING_MODEL_NAME)
EMBEDDING_DIMENSION = EMBEDDING_MODEL.get_sentence_embedding_dimension()

router=APIRouter()


def embed_documents_texts(texts: List[str]) -> Union[np.ndarray, None]:
    """Generates a single vector embedding for each document text."""
    if EMBEDDING_MODEL is None:
        print("ERROR: Embedding model is unavailable. Cannot generate vectors.")
        return None
           
    try:
        # Generate embeddings and convert to float32 (FAISS requirement)
        embeddings = EMBEDDING_MODEL.encode(
            texts, 
            convert_to_numpy=True, 
            show_progress_bar=False
        ).astype(np.float32) 
           
        return embeddings
        
    except Exception as e:
        print(f"ERROR: Failed during batch embedding generation: {e}")
        return None
    
def extract_pdf_text(pdf_bytes: bytes) -> str:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    full_text = ""
    for page in doc:
        full_text += page.get_text()
    doc.close()
    return full_text

def update_document_index(raw_text: str, doc_id: str) -> bool:
    # Validate input
    if not raw_text or not doc_id:
        print("WARNING: raw_text or doc_id missing. Index not updated.")
        return False
    print(f"INFO: Generating embedding for document {doc_id}...")
    new_embedding = embed_documents_texts([raw_text])
    if new_embedding is None:
        print("ERROR: Embedding failed.")
        return False

    # Load or create FAISS index
    if os.path.exists("faiss.bin"):
        faiss_index = faiss.read_index("faiss.bin")
    else:
        if EMBEDDING_DIMENSION == 0:
            print("FATAL: Embedding dimension not initialized.")
            return False
        faiss_index = faiss.IndexFlatL2(EMBEDDING_DIMENSION)
        print("INFO: Created new FAISS index.")
    # Load or create map.json
    if os.path.exists("map.json"):
        with open("map.json", "r") as f:
            try:
                mapping = json.load(f)
                if not isinstance(mapping, dict):
                    mapping = {}
            except json.JSONDecodeError:
                mapping = {}
    else:
        mapping = {}
    # Determine FAISS vector ID
    new_faiss_id = faiss_index.ntotal
    # Add embedding to FAISS index
    faiss_index.add(new_embedding)
    # Add mapping: faiss_id -> doc_id
    mapping[str(new_faiss_id)] = doc_id
    # Save FAISS index
    faiss.write_index(faiss_index, "faiss.bin")

    # Save mapping JSON
    with open("map.json", "w") as f:
        json.dump(mapping, f, indent=2)
        print(f"SUCCESS: Stored vector in faiss.bin at index {new_faiss_id}")
        print(f"SUCCESS: Stored mapping {new_faiss_id} -> {doc_id} in map.json")

    return True           


# system_doc_id = str(uuid.uuid4())
FAISS_INDEX_PATH = 'faiss_index.bin'
DOC_MAP_PATH = 'doc_map.json'
fnial_document=None
# EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
def process_pdf_document(pdf_bytes: bytes) -> dict:
    system_doc_id = str(uuid.uuid4())

    # Usage
    raw_text = extract_pdf_text(pdf_bytes)

    clause_instances: List[ClauseContent] = extract_clauses(raw_text)
    if not clause_instances:
        # Handle the error if Gemini failed to return valid JSON or extracted 0 clauses
        print("FATAL: LLM extraction failed or returned zero clauses.")
    # print(clause_instances)    
    else:
    # --- 2. LLM CALL 2: EXTRACT METADATA (Properties Structuring) ---
    # This gives us the dictionary of top-level properties.
        # system_doc_id = str(uuid.uuid4())
        metadata_result = extract_metadata(
            raw_text, 
            system_doc_id=system_doc_id
        )

        if "error" in metadata_result:
            print(f"Error extracting metadata: {metadata_result['message']}")
            final_document = None

        else:
            # --- 3. ASSEMBLE FINAL DOCUMENT (Bottom-Up Construction) ---
            
            # Convert the list of ClauseContent Pydantic instances into a list of Python dicts
            # Note: model_dump() (or .dict() in v1) handles the Pydantic-to-dict conversion.
            clauses_data_dicts = [c.model_dump() for c in clause_instances]

            # Assemble the final dictionary:
            final_document = {
                # Top-level properties extracted by extract_metadata
                **metadata_result, 
                
                # The full, original text is stored for document-level RAG/context search
                "full_text_context": raw_text,
                
                # The structured, editable content
                "clauses": clauses_data_dicts,
            }
            db_path="db.json"
            if final_document:
                doc_id = final_document["doc_id"]

                # Load the existing JSON object
                if os.path.exists(db_path):
                    with open(db_path, "r") as f:
                        try:
                            db = json.load(f)
                            if not isinstance(db, dict):
                                db = {}
                        except json.JSONDecodeError:
                            db = {}
                else:
                    db = {}

                # Insert or overwrite this doc_id object
                db[doc_id] = final_document
                # db = {doc_id: final_document}
                # Save back to file
                with open(db_path, "w") as f:
                    json.dump(db, f, indent=2)

                print("--- DOCUMENT SAVED TO db.json ---")
            else:
                print("--- DOCUMENT PROCESSING FAILED ---")

            return {"status": "Document processed and indexed successfully"}

          
@router.post('/ocr_upload')
async def ocr_upload(file: UploadFile = File(...)):
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Invalid file type. Please upload a PDF file.")
    
    pdf_bytes = await file.read()
    result = process_pdf_document(pdf_bytes)
    return result